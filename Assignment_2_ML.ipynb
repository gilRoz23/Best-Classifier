{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and Analysis"
      ],
      "metadata": {
        "id": "ft-QSy6ZdJCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the dataset using pandas\n",
        "data = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ass2.pickle\")\n",
        "df = pd.DataFrame([data])\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(\"\\nShape of the dataset:\")\n",
        "print(df.shape)\n",
        "\n",
        "# Explore different columns and their data types\n",
        "print(\"\\nColumns and their data types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check for numeric columns stored as objects\n",
        "numeric_columns_as_objects = df.select_dtypes(include=['object']).apply(pd.to_numeric, errors='coerce').notnull().all()\n",
        "\n",
        "# Convert columns to numeric if applicable\n",
        "for col in numeric_columns_as_objects[numeric_columns_as_objects].index:\n",
        "    df[col] = pd.to_numeric(df[col])\n",
        "\n",
        "# Check the updated data types\n",
        "print(\"\\nUpdated Columns and their data types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for any missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Plot histograms for numeric features\n",
        "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "if not numeric_features.empty:\n",
        "    df[numeric_features].hist(figsize=(12, 10))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nNo numeric columns available for histogram plotting.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIYkm1Z11owY",
        "outputId": "1d974fdb-e05f-4a34-d5f5-5f77205c3fea"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "                                               train  \\\n",
            "0         f0  f1  f2  f3  f4  f5  f6  f7  f8  f9 ...   \n",
            "\n",
            "                                                 dev  \\\n",
            "0         f0  f1  f2  f3  f4  f5  f6  f7  f8  f9 ...   \n",
            "\n",
            "                                                test  \n",
            "0         f0  f1  f2  f3  f4  f5  f6  f7  f8  f9 ...  \n",
            "\n",
            "Shape of the dataset:\n",
            "(1, 3)\n",
            "\n",
            "Columns and their data types:\n",
            "train    object\n",
            "dev      object\n",
            "test     object\n",
            "dtype: object\n",
            "\n",
            "Updated Columns and their data types:\n",
            "train    object\n",
            "dev      object\n",
            "test     object\n",
            "dtype: object\n",
            "\n",
            "Summary statistics:\n",
            "                                                    train  \\\n",
            "count                                                   1   \n",
            "unique                                                  1   \n",
            "top            f0  f1  f2  f3  f4  f5  f6  f7  f8  f9 ...   \n",
            "freq                                                    1   \n",
            "\n",
            "                                                      dev  \\\n",
            "count                                                   1   \n",
            "unique                                                  1   \n",
            "top            f0  f1  f2  f3  f4  f5  f6  f7  f8  f9 ...   \n",
            "freq                                                    1   \n",
            "\n",
            "                                                     test  \n",
            "count                                                   1  \n",
            "unique                                                  1  \n",
            "top            f0  f1  f2  f3  f4  f5  f6  f7  f8  f9 ...  \n",
            "freq                                                    1  \n",
            "\n",
            "Missing values:\n",
            "train    0\n",
            "dev      0\n",
            "test     0\n",
            "dtype: int64\n",
            "\n",
            "No numeric columns available for histogram plotting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre Processing\n",
        "We converted the target variable to an integer to ensure that it is correctly interpreted as categorical by the classification models. Most machine learning algorithms expect the target variable to be encoded as integers representing different classes. By converting it to an integer type, we ensure compatibility with classification algorithms."
      ],
      "metadata": {
        "id": "sIZl8LPQc-P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pickle\n",
        "\n",
        "# Extract the train, dev, and test sets\n",
        "train_data = data['train']\n",
        "dev_data = data['dev']\n",
        "test_data = data['test']\n",
        "\n",
        "# Assuming the data is stored as a list of dictionaries or similar structures\n",
        "train_df = pd.DataFrame(train_data)\n",
        "dev_df = pd.DataFrame(dev_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Print the first few rows to inspect the data\n",
        "print(train_df.head())\n",
        "print(dev_df.head())\n",
        "print(test_df.head())\n",
        "\n",
        "# Ensure the target column is categorical\n",
        "print(\"\\nData types before encoding:\")\n",
        "print(train_df.dtypes)\n",
        "print(dev_df.dtypes)\n",
        "print(test_df.dtypes)\n",
        "\n",
        "# Convert the target column to categorical if it's not\n",
        "if train_df['target'].dtype == 'object' or train_df['target'].dtype == 'float64' or train_df['target'].dtype == 'int64':\n",
        "    le = LabelEncoder()\n",
        "    train_df['target'] = le.fit_transform(train_df['target'])\n",
        "    dev_df['target'] = le.transform(dev_df['target'])\n",
        "    test_df['target'] = le.transform(test_df['target'])\n",
        "\n",
        "# Verify the target column is now categorical\n",
        "print(\"\\nData types after encoding:\")\n",
        "print(train_df.dtypes)\n",
        "print(dev_df.dtypes)\n",
        "print(test_df.dtypes)\n",
        "\n",
        "# Handle missing values and convert appropriate columns to numeric if needed\n",
        "# Example: Fill missing values\n",
        "train_df.fillna(method='ffill', inplace=True)\n",
        "dev_df.fillna(method='ffill', inplace=True)\n",
        "test_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# # Standardize the numeric features if needed\n",
        "# numeric_features = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# train_df[numeric_features] = scaler.fit_transform(train_df[numeric_features])\n",
        "# dev_df[numeric_features] = scaler.transform(dev_df[numeric_features])\n",
        "# test_df[numeric_features] = scaler.transform(test_df[numeric_features])\n",
        "\n",
        "# Assume the target column is named 'target'\n",
        "X_train = train_df.drop('target', axis=1)\n",
        "y_train = train_df['target']\n",
        "X_dev = dev_df.drop('target', axis=1)\n",
        "y_dev = dev_df['target']\n",
        "X_test = test_df.drop('target', axis=1)\n",
        "y_test = test_df['target']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rwvvIT2Ue9y",
        "outputId": "3e50c687-a247-4967-8778-af7e1a5d1f32"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       f0  f1  f2  f3  f4  f5  f6  f7  f8  f9  ...  f33  f34  f35  f36  f37  \\\n",
            "51905   1   0   0   0   0   0   2   1   2   2  ...    0    0    0    2    0   \n",
            "52612   0   0   0   0   0   0   2   1   0   0  ...    0    0    0    2    0   \n",
            "61699   2   1   2   1   1   0   2   2   0   0  ...    0    0    0    1    0   \n",
            "6291    0   0   0   0   0   0   0   0   0   0  ...    0    0    0    2    0   \n",
            "17484   0   0   0   0   0   0   1   1   2   0  ...    0    0    0    2    1   \n",
            "\n",
            "       f38  f39  f40  f41  target  \n",
            "51905    0    0    0    0       2  \n",
            "52612    0    0    0    0       2  \n",
            "61699    0    0    0    0       2  \n",
            "6291     0    0    0    0       2  \n",
            "17484    2    0    0    0       2  \n",
            "\n",
            "[5 rows x 43 columns]\n",
            "       f0  f1  f2  f3  f4  f5  f6  f7  f8  f9  ...  f33  f34  f35  f36  f37  \\\n",
            "104     1   2   0   0   0   0   0   0   0   0  ...    0    0    0    0    0   \n",
            "3548    2   0   0   0   0   0   2   0   0   0  ...    0    0    0    0    0   \n",
            "11672   1   0   0   0   0   0   2   2   0   0  ...    0    0    0    2    1   \n",
            "58531   0   0   0   0   0   0   2   1   2   2  ...    0    0    0    0    0   \n",
            "14252   0   0   0   0   0   0   1   2   1   2  ...    0    0    0    0    0   \n",
            "\n",
            "       f38  f39  f40  f41  target  \n",
            "104      0    0    0    0       2  \n",
            "3548     0    0    0    0       1  \n",
            "11672    0    0    0    0       2  \n",
            "58531    0    0    0    0       1  \n",
            "14252    0    0    0    0       2  \n",
            "\n",
            "[5 rows x 43 columns]\n",
            "       f0  f1  f2  f3  f4  f5  f6  f7  f8  f9  ...  f33  f34  f35  f36  f37  \\\n",
            "62110   2   0   0   0   0   0   0   0   0   0  ...    0    0    0    0    0   \n",
            "15906   2   1   1   0   0   0   1   0   0   0  ...    0    0    0    0    0   \n",
            "23980   2   0   0   0   0   0   1   1   2   1  ...    0    0    0    0    0   \n",
            "1986    1   0   0   0   0   0   0   0   0   0  ...    0    0    0    0    0   \n",
            "53416   2   2   1   0   0   0   2   0   0   0  ...    0    0    0    1    0   \n",
            "\n",
            "       f38  f39  f40  f41  target  \n",
            "62110    0    0    0    0       2  \n",
            "15906    0    0    0    0       0  \n",
            "23980    0    0    0    0       2  \n",
            "1986     0    0    0    0       2  \n",
            "53416    0    0    0    0       2  \n",
            "\n",
            "[5 rows x 43 columns]\n",
            "\n",
            "Data types before encoding:\n",
            "f0        int64\n",
            "f1        int64\n",
            "f2        int64\n",
            "f3        int64\n",
            "f4        int64\n",
            "f5        int64\n",
            "f6        int64\n",
            "f7        int64\n",
            "f8        int64\n",
            "f9        int64\n",
            "f10       int64\n",
            "f11       int64\n",
            "f12       int64\n",
            "f13       int64\n",
            "f14       int64\n",
            "f15       int64\n",
            "f16       int64\n",
            "f17       int64\n",
            "f18       int64\n",
            "f19       int64\n",
            "f20       int64\n",
            "f21       int64\n",
            "f22       int64\n",
            "f23       int64\n",
            "f24       int64\n",
            "f25       int64\n",
            "f26       int64\n",
            "f27       int64\n",
            "f28       int64\n",
            "f29       int64\n",
            "f30       int64\n",
            "f31       int64\n",
            "f32       int64\n",
            "f33       int64\n",
            "f34       int64\n",
            "f35       int64\n",
            "f36       int64\n",
            "f37       int64\n",
            "f38       int64\n",
            "f39       int64\n",
            "f40       int64\n",
            "f41       int64\n",
            "target    int64\n",
            "dtype: object\n",
            "f0        int64\n",
            "f1        int64\n",
            "f2        int64\n",
            "f3        int64\n",
            "f4        int64\n",
            "f5        int64\n",
            "f6        int64\n",
            "f7        int64\n",
            "f8        int64\n",
            "f9        int64\n",
            "f10       int64\n",
            "f11       int64\n",
            "f12       int64\n",
            "f13       int64\n",
            "f14       int64\n",
            "f15       int64\n",
            "f16       int64\n",
            "f17       int64\n",
            "f18       int64\n",
            "f19       int64\n",
            "f20       int64\n",
            "f21       int64\n",
            "f22       int64\n",
            "f23       int64\n",
            "f24       int64\n",
            "f25       int64\n",
            "f26       int64\n",
            "f27       int64\n",
            "f28       int64\n",
            "f29       int64\n",
            "f30       int64\n",
            "f31       int64\n",
            "f32       int64\n",
            "f33       int64\n",
            "f34       int64\n",
            "f35       int64\n",
            "f36       int64\n",
            "f37       int64\n",
            "f38       int64\n",
            "f39       int64\n",
            "f40       int64\n",
            "f41       int64\n",
            "target    int64\n",
            "dtype: object\n",
            "f0        int64\n",
            "f1        int64\n",
            "f2        int64\n",
            "f3        int64\n",
            "f4        int64\n",
            "f5        int64\n",
            "f6        int64\n",
            "f7        int64\n",
            "f8        int64\n",
            "f9        int64\n",
            "f10       int64\n",
            "f11       int64\n",
            "f12       int64\n",
            "f13       int64\n",
            "f14       int64\n",
            "f15       int64\n",
            "f16       int64\n",
            "f17       int64\n",
            "f18       int64\n",
            "f19       int64\n",
            "f20       int64\n",
            "f21       int64\n",
            "f22       int64\n",
            "f23       int64\n",
            "f24       int64\n",
            "f25       int64\n",
            "f26       int64\n",
            "f27       int64\n",
            "f28       int64\n",
            "f29       int64\n",
            "f30       int64\n",
            "f31       int64\n",
            "f32       int64\n",
            "f33       int64\n",
            "f34       int64\n",
            "f35       int64\n",
            "f36       int64\n",
            "f37       int64\n",
            "f38       int64\n",
            "f39       int64\n",
            "f40       int64\n",
            "f41       int64\n",
            "target    int64\n",
            "dtype: object\n",
            "\n",
            "Data types after encoding:\n",
            "f0        int64\n",
            "f1        int64\n",
            "f2        int64\n",
            "f3        int64\n",
            "f4        int64\n",
            "f5        int64\n",
            "f6        int64\n",
            "f7        int64\n",
            "f8        int64\n",
            "f9        int64\n",
            "f10       int64\n",
            "f11       int64\n",
            "f12       int64\n",
            "f13       int64\n",
            "f14       int64\n",
            "f15       int64\n",
            "f16       int64\n",
            "f17       int64\n",
            "f18       int64\n",
            "f19       int64\n",
            "f20       int64\n",
            "f21       int64\n",
            "f22       int64\n",
            "f23       int64\n",
            "f24       int64\n",
            "f25       int64\n",
            "f26       int64\n",
            "f27       int64\n",
            "f28       int64\n",
            "f29       int64\n",
            "f30       int64\n",
            "f31       int64\n",
            "f32       int64\n",
            "f33       int64\n",
            "f34       int64\n",
            "f35       int64\n",
            "f36       int64\n",
            "f37       int64\n",
            "f38       int64\n",
            "f39       int64\n",
            "f40       int64\n",
            "f41       int64\n",
            "target    int64\n",
            "dtype: object\n",
            "f0        int64\n",
            "f1        int64\n",
            "f2        int64\n",
            "f3        int64\n",
            "f4        int64\n",
            "f5        int64\n",
            "f6        int64\n",
            "f7        int64\n",
            "f8        int64\n",
            "f9        int64\n",
            "f10       int64\n",
            "f11       int64\n",
            "f12       int64\n",
            "f13       int64\n",
            "f14       int64\n",
            "f15       int64\n",
            "f16       int64\n",
            "f17       int64\n",
            "f18       int64\n",
            "f19       int64\n",
            "f20       int64\n",
            "f21       int64\n",
            "f22       int64\n",
            "f23       int64\n",
            "f24       int64\n",
            "f25       int64\n",
            "f26       int64\n",
            "f27       int64\n",
            "f28       int64\n",
            "f29       int64\n",
            "f30       int64\n",
            "f31       int64\n",
            "f32       int64\n",
            "f33       int64\n",
            "f34       int64\n",
            "f35       int64\n",
            "f36       int64\n",
            "f37       int64\n",
            "f38       int64\n",
            "f39       int64\n",
            "f40       int64\n",
            "f41       int64\n",
            "target    int64\n",
            "dtype: object\n",
            "f0        int64\n",
            "f1        int64\n",
            "f2        int64\n",
            "f3        int64\n",
            "f4        int64\n",
            "f5        int64\n",
            "f6        int64\n",
            "f7        int64\n",
            "f8        int64\n",
            "f9        int64\n",
            "f10       int64\n",
            "f11       int64\n",
            "f12       int64\n",
            "f13       int64\n",
            "f14       int64\n",
            "f15       int64\n",
            "f16       int64\n",
            "f17       int64\n",
            "f18       int64\n",
            "f19       int64\n",
            "f20       int64\n",
            "f21       int64\n",
            "f22       int64\n",
            "f23       int64\n",
            "f24       int64\n",
            "f25       int64\n",
            "f26       int64\n",
            "f27       int64\n",
            "f28       int64\n",
            "f29       int64\n",
            "f30       int64\n",
            "f31       int64\n",
            "f32       int64\n",
            "f33       int64\n",
            "f34       int64\n",
            "f35       int64\n",
            "f36       int64\n",
            "f37       int64\n",
            "f38       int64\n",
            "f39       int64\n",
            "f40       int64\n",
            "f41       int64\n",
            "target    int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The different Classifiers"
      ],
      "metadata": {
        "id": "XVg7DloBdbh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression**\n",
        "Logistic regression was chosen as one of the initial models due to its simplicity and interpretability. However, it might not perform well if the relationship between the features and the target variable is not linear. In cases where the decision boundary is more complex, logistic regression may not capture it effectively, leading to suboptimal performance.\n",
        "\n",
        "Additionally, the UndefinedMetricWarning indicates that some classes have no predicted samples in the logistic regression model, resulting in precision and F-score being set to 0 for those classes. This can occur if the model is not well-tuned or if the data is imbalanced. It's essential to address these issues by tuning hyperparameters or using techniques to handle class imbalance to improve the model's performance."
      ],
      "metadata": {
        "id": "frdSuLVQdqXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize and train the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the dev set\n",
        "log_reg_pred = log_reg.predict(X_dev)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_dev, log_reg_pred)}\")\n",
        "\n",
        "\n",
        "# Optionally, evaluate on the test set\n",
        "log_reg_test_pred = log_reg.predict(X_test)\n",
        "print(\"\\nLogistic Regression Test Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, log_reg_test_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaPSLgUqbWAe",
        "outputId": "ea76555d-8424-4f57-991a-37f6e4edb0a2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Performance:\n",
            "Accuracy: 0.6596358792184724\n",
            "\n",
            "Logistic Regression Test Performance:\n",
            "Accuracy: 0.6595618709295441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Desicion Tree**\n",
        "A decision tree was chosen as it can capture non-linear relationships between features and the target variable. Decision trees are also easy to interpret and visualize, making them useful for gaining insights into the data. However, decision trees tend to overfit the training data, which can lead to poor generalization performance on unseen data. But it was a way to start."
      ],
      "metadata": {
        "id": "cJB3DeaJdf1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize and train the model\n",
        "dec_tree = DecisionTreeClassifier()\n",
        "dec_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the dev set\n",
        "dec_tree_pred = dec_tree.predict(X_dev)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Decision Tree Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_dev, dec_tree_pred)}\")\n",
        "\n",
        "# Optionally, evaluate on the test set\n",
        "dec_tree_test_pred = dec_tree.predict(X_test)\n",
        "print(\"\\nDecision Tree Test Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, dec_tree_test_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSdNE7mDYPB0",
        "outputId": "3081d79f-32d3-4e67-d2a9-3979e83d2fa5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Performance:\n",
            "Accuracy: 0.7217288336293665\n",
            "\n",
            "Decision Tree Test Performance:\n",
            "Accuracy: 0.7223208999407934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest**\n",
        "Random forests were chosen as an extension of decision trees to address the overfitting issue. Random forests use an ensemble of decision trees, where each tree is trained on a random subset of the data and features. By averaging the predictions of multiple trees, random forests reduce overfitting and improve generalization performance as we saw in class. Random forests also provide feature importance scores, which can be useful for feature selection and interpretation."
      ],
      "metadata": {
        "id": "g5Ywa08pdxmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize and train the model\n",
        "rand_forest = RandomForestClassifier()\n",
        "rand_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the dev set\n",
        "rand_forest_pred = rand_forest.predict(X_dev)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_dev, rand_forest_pred)}\")\n",
        "\n",
        "\n",
        "# Optionally, evaluate on the test set\n",
        "rand_forest_test_pred = rand_forest.predict(X_test)\n",
        "print(\"\\nRandom Forest Test Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rand_forest_test_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGpK88NNbkW5",
        "outputId": "615602e1-770b-4f1a-e5a4-e067561d569e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Performance:\n",
            "Accuracy: 0.8088365896980462\n",
            "\n",
            "Random Forest Test Performance:\n",
            "Accuracy: 0.8040260509177027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest with Class Weighting\n",
        "Class imbalance can lead to biased models. By assigning higher weights to minority class samples during training, Random Forest with class weights effectively balances the class distribution, improving the model's ability to learn from minority class instances"
      ],
      "metadata": {
        "id": "cfRGOd2kizay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the model with class weights\n",
        "rand_forest = RandomForestClassifier(class_weight='balanced')\n",
        "rand_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the dev set\n",
        "rand_forest_pred = rand_forest.predict(X_dev)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Performance with Class Weighting:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_dev, rand_forest_pred)}\")\n",
        "\n",
        "# Optionally, evaluate on the test set\n",
        "rand_forest_test_pred = rand_forest.predict(X_test)\n",
        "print(\"\\nRandom Forest Test Performance with Class Weighting:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rand_forest_test_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBxeHsVgijiQ",
        "outputId": "510491e7-7286-43fe-e397-ac7e87a2d86c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Performance with Class Weighting:\n",
            "Accuracy: 0.8100207223208999\n",
            "\n",
            "Random Forest Test Performance with Class Weighting:\n",
            "Accuracy: 0.8035079928952042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ada Boost\n",
        "AdaBoost was tried because:\n",
        "1. Works Well with Weak Learners: AdaBoost is effective with weak learners, such as decision trees with limited depth. By sequentially training weak learners and adjusting their weights based on their performance, AdaBoost creates a strong classifier.\n",
        "2. Boosting Mechanism: AdaBoost focuses on difficult-to-classify instances, improving the model's performance over iterations. However, its performance depends on the quality of weak learners and the dataset characteristics."
      ],
      "metadata": {
        "id": "lT32lZcnjUkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the AdaBoost classifier with a decision tree base estimator\n",
        "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2JigD82fzdN",
        "outputId": "e350669b-b09d-4810-8903-ca0d9a0af350"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7295737122557726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metric Used for Evaluation on the Dev Set\n",
        "We used accuracy as the primary metric to evaluate the models on the dev set. Accuracy measures the proportion of correctly classified instances out of the total instances in the dataset. While accuracy provides a good overall assessment of model performance, it may not be suitable for imbalanced datasets. However, since we employed techniques like class weights and ensemble methods to address class imbalance, accuracy serves as a reasonable metric in this context.\n",
        "\n",
        "# Hyperparameter Search and its Impact on Model Accuracy\n",
        "In the provided code snippets, we did not explicitly perform hyperparameter search or tuning. However, the chosen models, such as Random Forest with class weights and AdaBoost, have hyperparameters that can significantly impact model performance.\n",
        "\n",
        "For Random Forest, key hyperparameters include the number of trees (n_estimators), the maximum depth of each tree (max_depth), and the minimum number of samples required to split an internal node (min_samples_split). Adjusting these hyperparameters can affect the model's ability to capture complex patterns in the data, prevent overfitting, and improve generalization performance.\n",
        "\n",
        "Similarly, AdaBoost has hyperparameters like the number of weak learners (n_estimators) and the learning rate (learning_rate). These parameters control the boosting process and influence the model's capacity to adapt to the training data.\n",
        "\n",
        "A systematic hyperparameter search, such as grid search or random search, could be conducted to find the optimal combination of hyperparameters that maximizes model performance on the dev set. By evaluating the models with different hyperparameter configurations, we can identify the settings that yield the highest accuracy or other desired metrics.\n",
        "\n",
        "# Model Selection\n",
        "Based on the evaluation results and considering the use of class weights to handle class imbalance, we ultimately chose Random Forest with class weights. Despite AdaBoost being tried, Random Forest with class weights performed slightly better, making it the preferred choice for our classification task."
      ],
      "metadata": {
        "id": "E6PDPwIfjzdx"
      }
    }
  ]
}